\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{tdo}{\contentsline {todo}{wie l\IeC {\"a}uft Training ab? Was wird vergessen, was durchgelassen, wie funktioniert BPPTT?}{1}{section*.1}}
\pgfsyspdfmark {pgfid1}{4661699}{40926674}
\pgfsyspdfmark {pgfid4}{38701418}{40908324}
\pgfsyspdfmark {pgfid5}{41132426}{40687796}
\@writefile{tdo}{\contentsline {todo}{V. 2 F. 23, V. 4 F. 21+23}{1}{section*.2}}
\pgfsyspdfmark {pgfid6}{7141117}{29711902}
\pgfsyspdfmark {pgfid9}{38701418}{29693552}
\pgfsyspdfmark {pgfid10}{41132426}{29473024}
\@writefile{tdo}{\contentsline {todo}{Early Stopping, Weight Decay, Label Smoothing (V. 2 F. 37)}{1}{section*.3}}
\pgfsyspdfmark {pgfid11}{4661699}{15663317}
\pgfsyspdfmark {pgfid14}{38701418}{15644967}
\pgfsyspdfmark {pgfid15}{41132426}{15424439}
\@writefile{tdo}{\contentsline {todo}{V. 2, F. 49: Output jeder Layer h\IeC {\"a}ngt von Statistiken der Inputs ab, diese ver\IeC {\"a}ndern sich mit jedem Trainingsschritt aufgrund ver\IeC {\"a}nderlicher Gewichte -> Layer muss lernen Effekt auszugleichen, aber instable Target. L\IeC {\"o}sung: Batch Normalization}{1}{section*.4}}
\pgfsyspdfmark {pgfid16}{4661699}{11239637}
\pgfsyspdfmark {pgfid19}{38701418}{11190423}
\pgfsyspdfmark {pgfid20}{41132426}{10969895}
\@writefile{tdo}{\contentsline {todo}{V. 2 F. 54: H\IeC {\"o}here Wahrscheinlichkeit auch sehr komplexe Funktionen zu lernen, ohne Gegenma\IeC {\ss }nahmen (Regularisierung) h\IeC {\"o}here Wahrscheinlichkeit des Overfittings. \IeC {\"U}berparametrisierung kann auch zu double-descent risk curve f\IeC {\"u}hren mit besserer Generalisierung, s. Belkin et al. 2019}{1}{section*.5}}
\pgfsyspdfmark {pgfid21}{4661699}{5832917}
\pgfsyspdfmark {pgfid24}{38701418}{-7849485}
\pgfsyspdfmark {pgfid25}{41132426}{-8070013}
\@writefile{tdo}{\contentsline {todo}{V 2, F. 61: Backward Gradient wird entlang Residual Connection additiv propagiert. }{2}{section*.6}}
\pgfsyspdfmark {pgfid26}{4661699}{46421355}
\pgfsyspdfmark {pgfid29}{38701418}{46403005}
\pgfsyspdfmark {pgfid30}{41132426}{46182477}
\@writefile{tdo}{\contentsline {todo}{V3 F4: Compress instances and generate similar ones, encoder and decoder structure}{2}{section*.7}}
\pgfsyspdfmark {pgfid31}{4661699}{39212395}
\pgfsyspdfmark {pgfid34}{38701418}{39022935}
\pgfsyspdfmark {pgfid35}{41132426}{38802407}
\@writefile{tdo}{\contentsline {todo}{Misst die Un\IeC {\"a}hnlichkeit einer Verteilung P von einer Referenzverteilung Q. Misst die erwartete zus\IeC {\"a}tzliche Shannon-Information, wenn Q anstelle der wahren Verteilung P verwendet wird.}{2}{section*.8}}
\pgfsyspdfmark {pgfid36}{4661699}{34788715}
\pgfsyspdfmark {pgfid39}{38701418}{31779507}
\pgfsyspdfmark {pgfid40}{41132426}{31558979}
\@writefile{tdo}{\contentsline {todo}{Super simpel mit $p_1 = 0.3, p_2 = 1-p_1 = 0.7, q_1=q_2=0.5$}{2}{section*.9}}
\pgfsyspdfmark {pgfid41}{4661699}{30365035}
\pgfsyspdfmark {pgfid44}{38701418}{14682085}
\pgfsyspdfmark {pgfid45}{41132426}{14461557}
\@writefile{tdo}{\contentsline {todo}{V3 F12: Verwende N(0,1) als Zielverteilung, das sorgt f\IeC {\"u}r simple Form der KL-Divergence, die differenzierbar ist. }{2}{section*.10}}
\pgfsyspdfmark {pgfid46}{4661699}{25941355}
\pgfsyspdfmark {pgfid49}{38701418}{9252037}
\pgfsyspdfmark {pgfid50}{41132426}{9031509}
\@writefile{tdo}{\contentsline {todo}{Unscharfe Bilder, L\IeC {\"o}sung z.B. VQ-VAE II}{2}{section*.11}}
\pgfsyspdfmark {pgfid51}{4661699}{20534635}
\pgfsyspdfmark {pgfid54}{38701418}{-940511}
\pgfsyspdfmark {pgfid55}{41132426}{-1161039}
\@writefile{tdo}{\contentsline {todo}{AE: nur reconstruction loss, VAE: reconstruction loss + variational loss als KL div, um latent codes zu N(0,1) zu pushen. Das erlaubt Generierung von neuen Elementen aus der Distribution durch samplen von Latent Codes aus Standardnormalverteilung.}{2}{section*.12}}
\pgfsyspdfmark {pgfid56}{4661699}{15127915}
\pgfsyspdfmark {pgfid59}{38701418}{-4377281}
\pgfsyspdfmark {pgfid60}{41132426}{-4597809}
\@writefile{tdo}{\contentsline {todo}{Generator generiert Daten basierend auf random input. Discriminator muss generierte und echte Daten unterscheiden.}{3}{section*.13}}
\pgfsyspdfmark {pgfid61}{4661699}{47895915}
\pgfsyspdfmark {pgfid64}{38701418}{47877565}
\pgfsyspdfmark {pgfid65}{41132426}{47657037}
\@writefile{tdo}{\contentsline {todo}{Joint optimization von zwei verschiedenen Netzwerken. Trainingsziele beider Netze m\IeC {\"u}ssen anspruchsvoll, aber nicht unm\IeC {\"o}glich sein. Discriminator dominates generator: generator lernt nicht wie er Daten generieren kann, die Discriminator \IeC {\"u}berlisten. Generator dominiert Discriminator: Discriminator \IeC {\"u}bt keinen Druck auf Generator aus bessere Daten zu erzeugen und dieser stagniert.}{3}{section*.14}}
\pgfsyspdfmark {pgfid66}{4661699}{42489195}
\pgfsyspdfmark {pgfid69}{38701418}{37685017}
\pgfsyspdfmark {pgfid70}{41132426}{37464489}
